{"cells":[{"cell_type":"markdown","source":[" ## Classification - Decision Trees\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## Setup\n","\n"," In this example, we will explore data from the titanic that comes from Kaggle (https://www.kaggle.com/c/titanic/data). You can view the attributes in the data from the link previous. The following set of code will install a couple a new packages that we will utilize for this section of the course, the titanic package has the data we will use and the rpart package includes functions to perform the tree based models we will employ.\n","\n"," ### Loading R packages"],"metadata":{}},{"source":[".libPaths('../RPackages')\n","\n","install.packages(c(\"titanic\", \"rpart\", \"caret\", \"rsample\", \"rpart.plot\"))\n","\n","library(tidyverse)\n","library(ggformula)\n","library(mosaic)\n","library(titanic)\n","library(rpart)\n","library(caret)\n","library(rpart.plot)\n","\n","theme_set(theme_bw())\n","\n","titanic <- bind_rows(titanic_train, titanic_test) %>% \n","  mutate(survived = ifelse(Survived == 1, 'Survived', 'Died')) %>% \n","  select(-Survived) %>%\n","  drop_na(survived)\n","\n","head(titanic)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Introduction to Decision Trees\n"," Decision trees is a method to predict an outcome based on decision rules and is named after the tree like structure that it creates. The method makes \"decisions\" based on the data to maximize some critera that we choose. Common criteria could be minimizing error or maximizing the correct predictions made. A picture and example may be helpful in this context to see an example of a decision tree. The New York Times published an interactive online quiz that asks a series of questions to explore if an individual is more likely to be a democrat or a republican. The questions are relatively simple and using a decision tree method attempts to predict the likelihood someone is a democrat or republican (https://www.nytimes.com/interactive/2019/08/08/opinion/sunday/party-polarization-quiz.html).\n"," ![decision-tree](../images/nytimes-decision-tree.png)"],"metadata":{}},{"cell_type":"markdown","source":[" ## Predict Survival\n"," Let's try to predict whether a passenger survived the titanic iceberg disaster. What may be some attributes that are important in predicting whether an individual survived the titanic disaster? What would be a good performance measure to understand how well the model does at identifying if a person survived the disaster?\n","\n"," Let's first look at how many individuals survived. For this, we can use the `count()` function. The first argument is the data, the second argument is the attribute we wish to count the number of occurances for each unique value."],"metadata":{}},{"source":["count(titanic, survived)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" As you can see, about 38% survived the disaster, 342 out of 891.\n","\n"," We can also create a bar chart that shows the number that survived."],"metadata":{}},{"source":["gf_bar(~ survived, data = titanic)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## Fitting a Classification Tree\n"," Let's class_tree our first classification tree to predict the dichotomous attribute, survival. For this, we will use the `rpart()` function from the rpart package. The first argument to the `rpart()` function is a formula where the outcome of interest is specified to the left of the `~` and the attributes that are predictive of the outcome are specified to the right of the `~` separated with `+` signs. The second argument specifies the method for which we want to run the analysis, in this case we want to classify individuals based on the values in the data, therefore we specify `method = 'class'`. The final argument is the data element, in this case titanic.\n","\n"," In this example, I picked a handful of attributes that would seem important. These can either be numeric or represent categories, the method does not care the type of attributes that are included in the analysis. Notice that I save the computation to the object, `class_tree`."],"metadata":{}},{"source":["class_tree <- rpart(survived ~ Pclass + Sex + Age + Fare + Embarked + SibSp + Parch, \n","   method = 'class', data = titanic)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Visualizing the model object can be a good way to understand what is happening with the model. Here we are using the `rpart.plot()` function to create a nicer looking visualization than the default. The primary argument for this function is the model object that was saved above."],"metadata":{}},{"source":["rpart.plot(class_tree, roundint = FALSE, type = 3, branch = .3)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" We can also print out a list of the decision rules based on the classification model using the `rpart.rules()` function. This function again takes the model object fitted above as the primary argument."],"metadata":{}},{"source":["rpart.rules(class_tree, cover = TRUE)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### Pruning Trees\n"," One downside of decision trees, is that they can tend to overfit the data and capitalize on chance variation in our sample that we can not generalize to another sample. This means that there are features in the current sample that would not be present in another sample of data. There are a few ways to overcome this, one is to prune the tree to only include the attributes that are most important and improve the classification accuracy. One measure of this can be used is called the complexity parameter (CP) and this statistic attempts to balance the tree complexity related to how strongly the levels of the tree improve the classification accuracy. We can view these statistics with the `printcp()` and `plotcp()` functions where the only argument to be specified is the classification tree computation that was saved in the previous step."],"metadata":{}},{"source":["printcp(class_tree)\n","plotcp(class_tree)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### Perform the Pruning\n"," To perform the pruning, the `prune()` function is used"],"metadata":{}},{"source":["prune_class_tree <- prune(class_tree, cp = class_tree$cptable[which.min(class_tree$cptable[,\"xerror\"]),\"CP\"])\n","\n","# plot the pruned tree and the decision rules\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["rpart.plot(prune_class_tree, roundint = FALSE, type = 3, branch = .3)\n","rpart.rules(prune_class_tree, cover = TRUE)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ### Accuracy\n"," We can explore model performance by looking at the percentage of correct classifications. Basically, does the classification tree accurately classify passengers as surviving vs not surviving. To do this, we use the model to apply the decision rules shown in the above figure to apply the survived or not classifications. This can be done quickly with R using the `predict()` function."],"metadata":{}},{"source":["titanic_predict <- titanic %>%\n","  mutate(tree_predict = predict(prune_class_tree, type = 'class')) %>%\n","  cbind(predict(prune_class_tree, type = 'prob'))\n","head(titanic_predict, n = 20)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" We can then create a table to show how the data differ based on the observed vs predicted values."],"metadata":{}},{"source":["titanic_predict %>%\n","  count(survived, tree_predict)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" This result can also be visualized."],"metadata":{}},{"source":["gf_bar(~ survived, fill = ~tree_predict, data = titanic_predict)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Normalizing the groups can be advantageous as this can help to show percentages more directly."],"metadata":{}},{"source":["gf_bar(~ survived, fill = ~tree_predict, data = titanic_predict, position = 'fill')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" Finally, to get a numeric quantity, we could then compute the percentage of classification accuracy."],"metadata":{}},{"source":["titanic_predict %>%\n","  mutate(same_class = ifelse(survived == tree_predict, 1, 0)) %>%\n","  df_stats(~ same_class, mean, sum)\n","\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2, "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 }}